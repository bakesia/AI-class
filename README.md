# AI-class
기계학습 프로그래밍 과목 학습용

## 인공지능 기초를 위한 FAQ


<details>
<summary><b>1. 인공지능에서 지능에 해당하는 기능은 무엇인가?</b></summary>
  A : 인간이 가진 지적 능력(학습, 추론, 창의성 등)을 기계가 수행하도록 하는 핵심 기능을 뜻함
  
  1. 지각(Perception) : 외부 환경을 인식하는 능력 (ex : 이미지, 음성 인식, 자연어 이해)
  2. 학습(Learning) : 데이터를 기반으로 경험에서 지식을 습득, 성능을 향상시키는 능력 (ex : 지도학습, 비지도학습, 강화학습)
  3. 추론(Reasoning) : 기존 지식, 규칙을 활용하여 새로운 결론을 도출하는 능력 (ex : 논리, 확률적, 인과 추론)
  4. 문제 해결(Problem Solving) : 주어진 목표를 달성하기 위해 전략을 세우고 실행하는 능력 (ex : 탐색 알고리즘, 최적화, 계획)
  5. 적응(Adaptation) : 새로운 상황이나 환경 변화에 맞춰 지식을 재구성하고 행동을 조정하는 능력 (ex : 온라인, 전이 학습, 메타러닝)
  6. 창의성(Creativity) : 기존의 지식과 아이디어를 결합해 새로운 아이디어, 해법을 만드는 능력 (ex : 생성형 ai)
  7. 사회적 지능(Social Intelligence) : 다른 존재(인간 or 기계)와 효과적으로 상호작용하는 능력 (ex : 대화형 ai, 감정 인식)
</details>
<details>
<summary><b>2. 인공지능의 종류 3가지에 대해서 설명하시오 (지도학습, 반지도학습, 강화학습)</b></summary>
  A : 
  
  1. 지도학습 (Supervised Learning)
  - 개념 : 입력 데이터와 그에 대응하는 정답이 함께 주어져, 모델이 입력 > 출력 관계를 학습하는 방식
  - 예시
    - 사진 > 이게 고양이 / 개 판별
    - 환자 데이터 > 혈압 수치 판별
  - 특징
    - 데이터에 정답이 반드시 있어야 한다
    - 분류, 회귀 문제에 주로 사용 된다
  - 장점
    - 정확한 예측 가능, 학습이 안정적
  - 단점
    - 대량의 라벨링된 데이터가 필요, 비용이 큼
  
  2. 반지도학습 (Semi-Supervised Learning)
  - 개념 : 일부 데이터만 레이블이 있고, 나머지는 레이블이 없는 데이터를 함께 활용하는 학습 방식
  - 예시
    - 학생 시험지 1000장 중 100장만 채점(레이블 유), 나머지 900장은 채점 X (레이블 무)
    - 100장을 바탕으로 나머지를 보완해 학습
  - 특징
    - 라벨링 비용을 줄일 수 있다
    - 실제 산업/의료 분야에서 많이 활용(의료 분야 대부분은 라벨이 없음)
  - 장점
    - 소량의 라벨로도 성능향상 가능
  - 단점
    - 잘못된 라벨이나 데이터 분포가 불균형하면 성능이 떨어짐
  
  3. 강화학습 (Reinforcement Learning)
  - 개념 : 에이전트가 환경 속에서 행동을 수행하고, 그 결과로 보상을 받아 보상을 최대화하는 방향으로 학습하는 방식
  - 예시
    - 체스/바둑 ai : 이기면 보상, 지면 벌점
    - 길찾기 로봇 : 목표에 도달하면 보상, 벽에 부딪히면 벌점
  - 특징
    - 정답 데이터가 미리 주어지지 않음
    - 시행착오를 통해 스스로 학습
  - 장점
    - 자율적인 의사결정 가능
    - 복잡한 문제 해결 가능
  - 단점
    - 학습 속도 느림
    - 보상 설계 어려움  
</details>
<details>
<summary><b>3. 전통적인 프로그래밍 방법과 인공지능 프로그램의 차이점은 무엇인가?</b></summary>
  A: 
  
  1. 전통적 프로그래밍 방법
  - 규칙 기반(명시적 지시) : 사람이 문제 해결 절차와 규칙(알고리즘)을 직접 정의
  - 입력 + 규칙 -> 출력 구조
  - 예시 : 세금 계산 프로그램 : 세율과 계산식 입력 -> 결과 출력
  - 장점
    - 예측 가능성, 디버깅이 용이
    - 결과가 항상 동일
  - 단점
    - 규칙이 복잡하거나 불명확한 문제(얼굴 인식, 자연어 처리)는 규칙을 정의하기 힘듦

  2. 인공지능 프로그램
  - 데이터 기반 학습 : 사람이 규칙을 작성하는게 아니라 데이터를 통해 컴퓨터가 스스로 규칙을 학습
  - 입력 + 출력(정답) -> 학습 -> 규칙(모델) 형성
  - 예시 : 고양이 사진과 답 알려주고 -> 학습 -> 고양이 유무 판별
  - 장점
    - 복잡하고 규칙을 정의하기 힘든 문제도 해결 가능
    - 새로운 패턴에도 스스로 적응 가능
  - 단점
    - 학습 데이터 품질에 따라 성능이 좌우
    - 내부 작동 원리가 불투명해질 수 있음(블랙박스 문제)
  
</details>
<details>
<summary><b>4. 딥러닝과 머신러닝의 차이점은 무엇인가?</b></summary>
  A: 
  
  - 머신러닝의 특징 추출
    - 사람이 직접 정의
    - 모델은 사람이 설계한 특징을 입력받아 학습
    - 따라서 성능은 특징 설계의 품질에 크게 의존
  - 딥러닝의 특징 추출
    - 자동 특징 학습
    - 원시 데이터에서 직접 특징을 학습
    - 신경망 계층의 깊이에 따라 학습률이 달라짐
    - 특징 설계에 사람의 개입이 거의 필요 X, 데이터만 충분하다면 스스로 특징 추출 가능
</details>
<details>
<summary><b>5. Classification과 Regression의 주된 차이점은?</b></summary>
  A: 

  - 분류(Classification)
    - 출력값 : 이산적(discrete)
    - 입력 데이터가 어느 카테고리에 속하는지를 판별
    - 예시
      - ppg로 스트레스 상태 분류 (정상, 경도, 중증)
      - 환자의 질병 판단 (있음, 없음)
     
  - 회귀(Regression)
    - 출력값 : 연속적(continuous)
    - 입력 데이터를 통해 숫자 크기나 양을 예측
    - 예시
      - ppg로 혈압 예측 (120, 140...)
      - 온도 예측 (36, 25...)
</details>
<details>
<summary><b>6. 머신러닝에서 차원의 저주(curse of dimensionality)란?</b></summary>
  A: 

  1. 차원이 늘어날수록 파라미터가 많아진다.
  - 2차원 선형회귀 : 가중치, 절편 하나씩만 찾으면 됨
  - 3차원 선형회귀 : 가중치 2개로
  - n차원 선형회귀 : 가중치 n개로
  - 즉, 차원이 올라갈수록 구해야 할 변수(파라미터)가 기하급수적으로 증가

  2. 데이터가 충분하지 않으면 해가 불안정해진다.
  - 파라미터(가중치)가 많아지면, 그걸 안정적으로 구하기 위해 데이터 수도 훨씬 많이 필요
  - 데이터는 부족한데, 차원만 높으면 여러 해가 존재하거나, 특정 방향으로 과적합 되어 버림

  3. 직관적 어려움
  - 2차원은 단순한 직선(선형 함수) 찾기
  - 3차원은 '평면' 찾기
  - n차원으로 갈수록 직관적, 수치적으로 안정적인 추정이 어려워 짐

  4. 정리
  - 차원이 늘어날수록 구해야 할 파라미터(가중치, 절편) 수가 기하급수적으로 증가, 그에 따른 요구 데이터 수도 증가 > 과적합 위험
  - 그래서 차원의 저주라고 부른다
</details>
<details>
<summary><b>7. 규제와 스케일링의 차이점? 각각의 목적은?</b></summary>
  A:

  1. 스케일링(Scaling)
  - 핵심 목적 : 특성 간 단위 차이를 보정해 수치적 안정성과 최적화 효율 확보
  - 전처리 단계
    - 표준화(Standardization) : 평균 0, 분산 1
    - 정규화(Normalization) : [0,1] 범위 변환
    - 데이터의 크기/범위를 맞추어 균형 있는 학습 보장
  - 모델 내부
    - Batch Normalization, Layer Normalization, RMSNorm
    - 학습 중간 표현을 정규화하여 기울기 폭주·소실 방지, 안정적 수렴
  - 효과 : 경사하강법 수렴 가속, 거리 기반 모델의 공정성 확보
  - 비유 : 경기 시작 전에 모든 선수를 같은 출발선에 세워두는 것

  2. 규제 (Regularization)
  - 핵심 목적 : 모델이 훈련 데이터에 과도하게 적합하지 않도록 복잡도를 제어하고 일반화 성능을 확보
  - 전처리 단계
    - Data Augmentation: 데이터에 변형(회전, 잡음, 마스킹) 추가 → 모델이 덜 민감해짐
    - Feature Selection: L1 기반 변수 선택, PCA 등 차원 축소 → 단순하고 의미 있는 피처만 사용
    - 데이터/피처 수준에서 불필요한 변수를 줄여 규제 효과
    - 모델 내부
      - L1 규제 (Lasso): 일부 가중치를 0으로 → 피처 제거 효과
      - L2 규제 (Ridge): 모든 가중치를 줄여서 안정화 → 피처 중요도 축소
      - Elastic Net, Dropout, Early Stopping, Weight Decay 등
      - 파라미터 공간에서 자유도를 줄여 일반화 성능 향상
  - 비유 : 경기 중 코치가 선수들을 통제 > 불필요한 선수 제외(L1), 영향력이 큰 선수 조율(L2)
</details>
<details>
<summary><b>8. Overfitting vs. Underfitting 목적과 차이는?</b></summary>
  A: 

  1. Overfitting (과적합)
  - 정의 : 모델이 훈련 데이터에 지나치게 적합하여, 훈련 데이터에서는 성능이 높지만, 새로운 데이터에서는 성능이 떨어지는 현상
  - 원인 : 모델이 너무 복잡하거나 파라미터가 많아 데이터의 노이즈(잡음)까지 학습하기 때문에
  - 목적 : 일반화 성능 확보 실패 > 데이터 패턴이 아니라 우연한 특성(노이즈)까지 외워버림
  - 특징
    - 훈련 오차 작음
    - 테스트 오차 매우 큼
    - 복잡한 모델에서 자주 발생

  2. Underfitting (과소적합)
  - 정의 : 모델이 너무 단순하여 데이터의 중요한 패턴조차 제대로 학습하지 못하는 현상
  - 원인 : 모델의 복잡도가 너무 낮거나 학습이 충분히 이뤄지지 않은 경우 발생
  - 목적 : 데이터 표현 부족 > 중요한 구조조차 잡아내지 못함
  - 특징
    - 훈련 오차 큼
    - 테스트 오차도 큼
    - 단순한 모델에서 자주 발생
</details>
<details>
<summary><b>9. Feature Engineering과 Feature Selection의 차이점은?</b></summary>
  A: 

  1. Feature Engineering (특징 공학)
  - 데이터를 가공해서 새로운 특징을 만들어내는 것
  - 예시 : 성적 데이터(원 데이터)를 가지고 새로운 특징 제작(평균, 총점, 편차 등)
  - 즉, 기존 데이터를 조합, 변환하여 새로운 정보를 뽑아내는 과정

  2. Feature Selection (특징 선택)
  - 여러 특징 중에서 중요한 것만 선택하는 것
  - 예시 : 성적 데이터(원 데이터)에서 대학 입시에 중요한 국, 영, 수 데이터만 선택
  - 즉, 불필요한 특징은 줄이고 핵심만 선택하는 과
</details>
<details>
<summary><b>10. 전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)</b></summary>
  A: 

  - 목적
    - 데이터 품질 향상 : 잘못된 값, 잡음을 처리하여 신뢰성있는 입력 확보
    - 모델 성능 개선 : 불필요한 변동 줄이고, 중요한 패턴이 더 잘 드러나도록 하기 위해
    - 일반화 성능 확보 : 데이터의 잡음 때문에 생기는 과적합 위험도 감소
    - 계산 효율성 향상 : 불필요한 이상치, 결측치를 줄여 학습 속도와 안정성 확보
  - 주요 전처리 방법
  - 노이즈 처리
    - 목적 : 불필요한 랜덤 노이즈를 줄여 패턴이 더 잘 드러나게 함
    - 방법
      - 평활화(Smoothing): 이동평균(Moving Average), Low-pass Filter
      - 변환: 로그 변환, 정규화로 극단값 완화
      - 신호 처리: 필터링(Fourier, Wavelet 등)
  - 이상치 처리
    - 목적 : 정상 범위를 벗어난 값이 모델을 왜곡하지 않도록 조정
    - 방법
      - 탐지 : 통계 기반 → IQR(사분위 범위), Z-score / 시각화 → Boxplot, Scatter plot
      - 처리 : 제거 (Delete) / 대체 (평균/중앙값으로 치환) / 변환 (로그, 스케일링으로 완화)
  - 결측치 처리
    - 목적 : 데이터 누락으로 인한 정보 손실 최소화
    - 방법
      - 단순 대체: 평균, 중앙값, 최빈값으로 채우기
      - 고급 대체: KNN Imputation, Regression, MICE 등
      - 제거: 결측치가 너무 많은 행/열은 삭제
      - 예측 기반 대체: ML/DL 모델로 결측치 예측
</details>
<details>
<summary><b>11. EDA(Explorary Data Analysis)란? 데이터의 특성 파악(분포, 상관관계)</b></summary>
  A: 

  EDA란
  - 정의 : 데이터셋을 다양한 통계적/시각적 방법으로 탐색하여 데이터의 구조, 분포, 관계, 이상치 등을 파악하는 과정
  - 목적
    - 데이터의 전반적인 특성을 이해
    - 노이즈, 이상치, 결측치 탐지
    - 변수 간 관계와 패턴 발견
    - 이후 모델링 방향과 전처리 전략을 결정하기 위해

  주요분석내용
  1. 데이터 분포(Distribution)
     - 목적 : 값들이 어떤 형태롤 분포하는지 확인
     - 방법
        - 히스토그램(Histogram)
        - 커널 밀도 추정(KDE Plot)
        - Boxplot (이상치 탐지 포함)
  2. 변수 간 상관관계 (Correlation)
     - 목적: 변수들이 서로 어떤 관계를 가지는지 파악
     - 방법
       - 상관계수(Correlation Coefficient, Pearson/Spearman)
       - Heatmap 시각화
       - Scatter plot (산점도)
  3. 데이터 품질 확인
     - 결측치(Missing Values) 비율
     - 이상치(Outliers) 존재 여부
     - 범주형 변수의 분포(빈도표, bar chart)
</details>
<details>
<summary><b>12. 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관되는가?</b></summary>
  A: 

  - 절편 : 선의 출발점 > 선이 y축과 만나는 위치
  - 기울기 : 선의 경사 > 입력이 1 늘면 출력이 얼마나 변하는지

  딥러닝과의 연관
  - 가중치 : 기울기와 같은 역할 > 입력 특징이 결과에 얼마나 중요한지를 조절
  - 바이어스 : 절편과 같은 역할 > 기준선을 움직여 더 잘 맞추도록 조정
  - 다만, 회귀에서는 직선 하나로 추세를 설명하지만 딥러닝에서는 수많은 기울기, 절편(파라미터) 값을 사용해 보다 복잡한 추세를 잡아냄
  
</details>
<details>
<summary><b>13. 결정트리에서 불순도(Impurity) – 지니 계수(Gini Index)란 무엇인가?</b></summary>
  A: 

  - 지니 계수 정의
  - 공식 : $Gini(t) = 1 - \sum_{i=1}^{C} p_i^2$
    - $p_i$ : 노드 t안에서 클래스 i가 나타날 확률 | $C$ : 클래스 개수
    - $p_i^2$ : 틀정 클래스 하나만 뽑힐 확률
    - 이를 모두 더하면 '한 클래스에만 치우쳐 있는 정도'를 나타냄
    - 따라서 1에서 이를 빼면 여러 클래스가 섞여있는 정도(불순도)를 얻을 수 있음
  - 값의 해석
    - 0 : 완전히 순수한 노드 (클래스가 하나)
    - 최대값 : 클래스가 균등하게 섞임 (이진 분류에서는 0.5가 최대)
    - 즉, 값이 낮을수록 더 순수한 노드
  - 역할
    1. 분할 기준
       - 결정트리는 데이터를 나눌 때 **지니 계수 감소량(Gini Gain)**을 최대화하는 분할을 찾음
       - 즉, 자식 노드들이 부모 노드보다 순수해지도록 나누게 함
    2. 분류 품질 평가
       - 어떤 특성을 기준으로 데이터를 나눴을 때, 클래스가 얼마나 잘 분리되는지 측정
    3. 과적합 방지
       - 지니 계수는 단순히 정확도(Accuracy)만 보는 것보다 더 정교하게 분할 품질을 평가하므로, 의미 없는 분할을 줄여즘
</details>
<details>
<summary><b>14. 서포트 벡터머신, 마신, 결정경계란? SVM 원리?</b></summary>
  A: 

  1. 서포트 벡터 머신(Support Vector Machine, SVM)
     - 정의: 분류(Classification)와 회귀(Regression) 문제에서 자주 사용하는 지도학습 알고리즘
     - 핵심 아이디어
       - 데이터를 두 클래스 이상으로 나눌 때, 클래스 간의 **결정 경계(Decision Boundary)**를 찾는 것
       - 이때 단순히 맞게 분류하는 것뿐 아니라 **클래스 간 간격(Margin)**을 최대화하는 경계를 찾는 것이 목표
  2. 마진(Margin)
     - 정의 : 결정 경계와 가장 가까운 데이터 포인트(서포트 벡터, Support Vectors) 사이의 거리
     - SVM은 이 마진을 최대화하는 결정 경계를 탐색
     - 이렇게 해야 새로운 데이터가 들어와도 일반화 성능(Generalization)이 좋아짐
     - 즉, 마진이 크면 모델이 '안정적으로' 분류가 가능함
  3. 결정 경계(Decision Boundary)
     - 정의 : 데이터를 다른 클래스로 구분하는 선(2D), 면(3D), 혹은 초평면(N차원)
     - 단순히 데이터를 구분하는 선이라기보다, 마진을 최대화하는 최적의 선/면을 의미
  4. SVM의 원리
     - 선형 분리 가능한 경우
       - 두 클래스가 직선(혹은 초평면)으로 정확히 구분될 수 있다면, SVM은 가능한 모든 직선 중에서 마진이 최대가 되는 직선을 탐색
       - 이 직선을 만드는 데 영향을 주는 데이터 포인트들이 바로 서포트 벡터
     - 선형 분리 불가능한 경우 (Soft Margin SVM)
       - 실제 데이터는 완벽히 나눌 수 없는 경우가 다수
       - 이때는 일부 오분류를 허용하면서도 마진을 최대화하도록 조정
       - 이를 Soft Margin이라 함
     - 비선형 분리 (Kernel Trick)
       - 원래 차원에서 직선으로 분리가 불가능하면, 데이터를 더 높은 차원으로 매핑함
       - 예: 2차원 원형 분포를 3차원으로 옮기면 선형 분리가 가능해짐
       - 이 매핑을 효율적으로 하기 위해 **커널 함수(Kernel Function, RBF, Polynomial 등)**를 사용함
</details>
<details>
<summary><b>15. 앙상블이란 무엇인가?</b></summary>
  A: 

  - 앙상블이란 여러 개의 서로 다른 모델을 결합해서 하나의 최종 예측 결과를 만드는 방법을 말함
    - 단일 모델 하나만 사용하는 대신, 여러 모델의 장점을 합쳐 더 좋은 성능(정확도, 일반화 능력 등)을 얻는 기법
    - 각각의 개별 모델은 저마다의 장단이 이씩에 여러 모델을 합쳐 약점을 보완, 과적합을 줄임
  - 주요 앙상블 방법
    1. 배깅(Bagging, Bootstrap Aggregating)
       - 여러 모델을 병렬로 학습
       - 각 모델은 원 데이터셋에서 부트스트랩 샘플링으로 조금씩 다른 데이터를 학습
       - 최종 예측은 다수결(분류)이나 평균(회귀)으로 결정
       - 대표 알고리즘: 랜덤 포레스트(Random Forest)
    2. 부스팅(Boosting)
       - 모델을 순차적으로 학습
       - 앞선 모델이 틀린 부분을 다음 모델이 집중해서 학습
       - 약한 학습기(weak learner)를 차례대로 강화
       - 대표 알고리즘: AdaBoost, Gradient Boosting, XGBoost, LightGBM
    3. 스태킹(Stacking)
       - 여러 다른 모델의 예측 결과를 메타 모델(meta-model) 이 종합하여 최종 예측
       - 예: 로지스틱 회귀, SVM, 신경망 등을 조합해 또 다른 모델로 합성
</details>
<details>
<summary><b>16. 부트 스트랩핑(bootstraping)이란 무엇인가?</b></summary>
  A: 

  - 통계적 추정이나 머신러닝 모델의 일반화 성능을 평가하기 위해 데이터를 “복원 추출(With Replacement)” 방식으로 무작위 샘플링하는 기법
  - 원리
    - 원 데이터셋 크기가 n개라고 할 때, 부트스트랩 샘플링은 같은 크기 n 만큼 무작위로 샘플을 추출
    - 이때 복원 추출을 하기 때문에 같은 데이터가 여러 번 선택될 수도 있고, 어떤 데이터는 아예 선택되지 않을 수도 있음
    - 즉, 원 데이터셋에서 만들어진 새로운 “가상 데이터셋(bootstrap sample)”을 구성하는 것
  - 특징
    1. 복원 추출(With Replacement)
       - 표본 하나를 뽑고 난 뒤 다시 돌려놓기 때문에, 뽑힌 데이터가 여러 번 포함될 수 있음
    2. 여러 번 반복
       - 부트스트랩 샘플링을 여러 번 반복하면, 원 데이터셋으로부터 다양한 분포를 반영한 샘플들을 얻을 수 있음
    3. 통계량 추정
       - 평균, 분산, 회귀계수 같은 추정치의 분포와 신뢰구간을 구하는 데 사용됨
    4. 머신러닝 활용
       - 앙상블 기법(Bagging, Random Forest): 각 부트스트랩 샘플로 모델을 학습한 뒤 결과를 평균/투표하여 일반화 성능 향상
       - 오차 추정(Out-of-Bag, OOB error): 샘플링 과정에서 선택되지 않은 데이터로 모델 성능을 검증
</details>
<details>
<summary><b>17. 선형회귀와 로지스틱 회귀의 차이점은?</b></summary>
  A:

  1. 선형회귀 (Linear Regression)
     - 목적 : 연속적인 값(실수)을 예측
     - 예: 집값, 키, 혈압, 주가 예측 등
     - 출력: 실수 범위의 연속적인 값
     - 모델식 : $y= w_1x_1 + w_2x_2 + ... + b$
     - 입력 변수의 선형 조합으로 출력값을 직접 예측
  2. 로지스틱 회귀 (Logistic Regression)
     - 목적: 범주형 값(특히 이진 분류)을 예측
     - 예: 스팸 메일 여부(Yes/No), 질병 유무(0/1)
     - 출력: 0~1 사이의 확률값
     - 모델식 : $P(y=1|x) = σ(w_1x_1 + w_2x_2 + ... + b)$
       - 여기서 σ는 시그모이드 함수로, 선형식을 확률로 변환
     - 결정 방법: 확률이 0.5 이상이면 클래스 1, 아니면 0으로 분류
  - 결론 : 선형회귀는 숫자 예측(얼마나), 로지스틱 회귀는 분류용(y or n, 0 or 1)
</details>
<details>
<summary><b>18. 경사하강법 의미는?</b></summary>
  A: 

  - 머신러닝과 딥러닝에서 모델이 학습할 때 사용하는 대표적인 최적화 알고리즘
    - 쉽게 말해, 오차(손실 함수, Loss Function)를 줄여가면서 최적의 파라미터(가중치와 바이어스)를 찾아가는 방법
  - 기본 구성
    1. 손실 함수(Loss function)
       - 모델의 예측값과 실제값 사이의 차이를 수치로 표현한 함수
       - 우리가 줄이고 싶은 대상(오차의 크기)
    2. 기울기(Gradient)
       - 손실 함수를 가중치에 대해 미분한 값
       - 기울기가 가리키는 방향은 오차가 가장 가파르게 증가하는 방향을 뜻함
    3. 내려가는 방식
       - 오차를 줄이고 싶으므로, 기울기의 반대 방향으로 가중치를 조금씩 업데이트함
       - 수식 : $w = w - a * \frac{∂L}{∂w}$
         - w : 가중치
         - L : 손실 함수
         - a : 학습률
    - 종류
      1. 배치 경사하강법 (Batch Gradient Descent)
         - 전체 데이터를 한 번에 계산 → 안정적이지만 느림
      2. 확률적 경사하강법 (SGD, Stochastic Gradient Descent)
         - 데이터를 한 개씩 사용 → 빠르지만 출렁거림이 큼
      3. 미니배치 경사하강법 (Mini-batch Gradient Descent)
         - 데이터를 여러 개 묶음(배치)으로 사용 → 속도와 안정성을 모두 챙김. (실제로 가장 많이 사용)
</details>
<details>
<summary><b>19. 데이터 불균형이 미치는 영향</b></summary>
  A: 
  
  1. 정확도 착시(Accuracy Illusion)
     - 불균형 데이터에서는 다수 클래스만 예측해도 정확도가 높게 나옴
     - 예: "정상 95%, 이상 5%" 데이터에서 모델이 전부 "정상"이라고만 해도 정확도는 95% → 겉보기엔 좋아 보임
  2. 소수 클래스 무시(Rare Class Ignored)
     - 모델이 학습 과정에서 드문 클래스(이상, 질병, 오류 등)를 거의 배우지 못합니다
     - 실제 중요한 결과(이상 탐지, 질환 판별 등)를 놓칠 가능성이 큼
  3. 일반화 성능 저하
     - 훈련 데이터에서는 잘 맞는 것 같아도, 새로운 데이터에서는 소수 클래스를 더 못 맞추는 경우가 많음
  4. 평가 지표 왜곡
     - Accuracy만 보면 성능이 좋아 보이지만, Precision, Recall, F1-Score, ROC-AUC 등으로 보면 실제 성능이 낮게 나타남
</details>
<details>
<summary><b>20. 훈련과 테스트 데이터로 나누는 까닭은? 일반화란 무엇인가?</b></summary>
  A: 

  1. 훈련(Training)과 테스트(Testing) 데이터를 나누는 이유
     - 훈련 데이터: 모델이 입력과 정답(레이블)을 보면서 규칙을 학습하는 데 사용됨
     - 테스트 데이터: 모델이 한 번도 보지 못한 새로운 데이터로, 모델이 실제 상황에서 얼마나 잘 작동하는지를 확인하기 위해 사용됨
     - 훈련 데이터만으로 평가하기엔 모델은 정답을 단순히 외워서 해석할 수 있음. 따라서 새로운 데이터를 사용해 모델이 답을 암기한 것이 아닌 규칙을 배웠는지를 확인해야 함
  2. 일반화(Generalization)란?
     - 정의: 모델이 훈련 때 보지 못한 새로운 데이터에서도 좋은 성능을 내는 능력을 말함
     - 즉, 단순히 훈련 데이터에만 맞춘 것이 아니라, 데이터의 본질적인 패턴을 이해했음을 의미
     - 예시
       - 학생이 시험 대비를 위해 문제집의 답만 외우면 모의고사(=훈련 데이터)는 잘 풀겠지만, 실제 시험(=테스트 데이터)에서는 낯선 문제가 나오면 못 품
       - 반대로 개념을 이해한 학생은 새로운 문제 유형이 나와도 잘 해결할 수 있
</details>
<details>
<summary><b>21. 교차검증(Cross Validation)이란?</b></summary>
  A: 

  - 머신러닝 모델의 성능을 더 신뢰성 있게 평가하기 위해 데이터를 여러 부분으로 나누어 반복적으로 학습과 검증을 수행하는 방법
  - 기본 아이디어
    -  데이터를 단순히 훈련(Train) / 테스트(Test) 한 번만 나누면, 분할 방식에 따라 성능 평가가 달라질 수 있음
    -  교차 검증은 데이터를 여러 개의 Fold(부분집합) 으로 나누고, 번갈아가며 검증에 사용
    -  따라서 평가의 편향(bias)을 줄이고 일반화 성능을 더 정확히 측정함
  - 가장 대표적인 방식 (K-Fold Cross Validation)
    1. 전체 데이터를 K개로 균등하게 분할
    2. K번의 반복에서:
       - (K-1)개 폴드는 학습에 사용
       - 남은 1개 폴드는 검증에 사용
    3. 모든 폴드가 검증에 한 번씩 사용된 후, 평균 성능을 최종 점수로 계산함
    - 예시 (5-Fold Cross Validation)
      - 데이터 5등분 → 1번째는 검증, 나머지 4개는 학습 → 성능 기록
      - 2번째는 검증, 나머지는 학습 → 성능 기록 …
      - 마지막까지 반복 → 5개 점수의 평균을 모델 성능으로 삼음
  - 다른 변형들
    - Stratified K-Fold: 분류 문제에서 클래스 비율이 고르게 유지되도록 나눔
    - Leave-One-Out (LOO): 데이터 한 개를 검증, 나머지를 학습으로 (데이터가 적을 때)
    - Repeated K-Fold: K-Fold를 여러 번 무작위 분할해서 반복
</details>
<details>
<summary><b>22. 평가 지표(Evaluation Metrics) 종류와 차이점은?</b></summary>
  A: 

  1. 회귀(Regression) 문제에서의 평가 지표
     - MSE (Mean Squared Error)
       - 예측값과 실제값의 차이를 제곱해서 평균한 값. 큰 오차에 더 민감함
     - RMSE (Root Mean Squared Error)
       - MSE에 제곱근을 취한 값. 실제 단위와 동일해 해석이 직관적
     - MAE (Mean Absolute Error)
       - 오차의 절댓값 평균. 큰 오차와 작은 오차를 균등하게 취급함
     - R² (결정계수)
       - 모델이 데이터를 얼마나 잘 설명하는지(분산 대비 설명력)를 나타냄
     - 차이점
       - MSE/RMSE는 큰 오차에 민감, MAE는 균형적, R²는 상대적 설명력 평가에 적합함
  2. 분류(Classification) 문제에서의 평가 지표
     -  Accuracy (정확도)
       - 전체 샘플 중 맞춘 비율. 데이터 불균형이 심하면 신뢰도 떨어짐
     - Precision (정밀도)
       - 양성이라고 예측한 것 중 실제 양성 비율. (False Positive를 줄이는 데 초점)
     - Recall (재현율)
       - 실제 양성 중 모델이 맞춘 비율. (False Negative를 줄이는 데 초점)
     - F1-score
       - Precision과 Recall의 조화평균. 불균형 데이터에서 균형 잡힌 성능 평가
     - ROC-AUC
       - 분류 임계값 변화에 따른 성능을 곡선으로 평가. 1에 가까울수록 성능 우수
     - PR-AUC
       - 특히 불균형 데이터에서 Precision-Recall 관계 평가
     - 차이점
       - Accuracy는 데이터 불균형에 약함, Precision/Recall은 FP/FN 상황에 따라 중요도가 달라짐
       - F1은 균형 지표, ROC-AUC/PR-AUC는 임계값 독립적으로 전반 성능을 보여줌
  3. 순위·추천(Re-ranking, Recommendation) 문제에서의 평가 지표
     - Top-k Accuracy: 정답이 상위 k개 예측 안에 포함되는 비율
     - MAP (Mean Average Precision): 랭킹 전체에서의 평균 정밀도
     - NDCG (Normalized Discounted Cumulative Gain): 순위에 따라 가중치가 다르게 반영되는 정규화된 지표
  4. 기타
     - 클러스터링 지표: 실루엣 계수, ARI, NMI 등 (라벨 없이 군집 품질 평가
     - 시계열/예측 지표: MAPE (Mean Absolute Percentage Error), SMAPE, DTW 거리 등
     - 의료/신호처리: Bland–Altman plot, Sensitivity/Specificity, Cohen’s Kappa 등
</details>
<details>
<summary><b>23. Accuracy말고 F1 을 사용하는 이유?</b></summary>
  A: 

  - Accuracy (정확도)
    - 수식 : $Accuracy = \frac{TP+TN}{TP+TN+FP+FN}​$
    - 전체 중에서 맞춘 비율
    - 하지만 데이터가 불균형할 때(예: 정상 95%, 질병 5%) → 모델이 무조건 "정상"이라고만 해도 정확도는 95%가 나옴
    - 즉, 소수 클래스(질병 같은 중요한 경우)를 무시해도 높은 값이 나올 수 있음
  - F1-score
    - 수식 : $F1 = 2 * \frac{Precision * Recall​}{Precision + Recall​}$
    - Precision(정밀도) = 모델이 양성이라 한 것 중 실제 양성 비율
    - Recall(재현율) = 실제 양성 중 모델이 맞춘 비율
    - F1은 Precision과 Recall의 균형을 본 것
    - 한쪽만 높고 다른 쪽이 낮으면 F1은 낮게 나옴 → 두 지표를 동시에 고려해야 함
  - F1을 쓰는 이유
    - 불균형 데이터 상황에서 Accuracy는 착시 효과를 줄 수 있음
    - F1은 “양성을 잘 맞추는 능력”을 더 공정하게 평가
    - 특히 의료 진단, 사기 탐지처럼 소수 클래스가 더 중요한 경우 F1이 훨씬 신뢰성 있는 지표
</details>
<details>
<summary><b>24. 차원 축소(Dimensionality Reduction) 기법은 무엇인가?</b></summary>
  A: 

  - 데이터의 중요한 정보를 최대한 보존하면서, 원래의 고차원 데이터를 더 적은 수의 차원(특성, feature)으로 표현하는 기법
  - 데이터의 차원이 커질수록(수백, 수천 개 특성) 계산량과 저장공간이 늘어나고, 모델이 과적합될 위험도 커지는데, 이를 줄이기 위해 차원 축소를 사용함
  - 주요 목적
    1. 시각화
    2. 노이즈 제거
    3. 연산 효율성 향상
    4. 과적합 방지
  - 대표적 기법
    1. 주성분 분석 (PCA, Principal Component Analysis)
       - 데이터의 분산을 가장 잘 설명하는 방향(주성분)을 찾아 투영하는 선형 차원 축소 기법(많이 쓰이는 대표 방법)
    2. 선형판별분석 (LDA, Linear Discriminant Analysis)
       - 클래스 간 분리를 잘 할 수 있는 축을 찾는 방법으로, 주로 분류 문제에 활용됨
    3. 비선형 차원 축소 기법
       - t-SNE (t-distributed Stochastic Neighbor Embedding): 데이터의 지역 구조(가까운 점들 사이의 관계)를 잘 보존, 시각화에 주로 사용
       - UMAP (Uniform Manifold Approximation and Projection): t-SNE보다 빠르고 큰 데이터에도 적용 가능
    5. 특성 선택 (Feature Selection)
       - 완전히 새로운 축으로 변환하는 것이 아니라, 기존 특성 중에서 중요한 것만 고르는 방식    
</details>
<details>
<summary><b>25. KNN, K-means 차이점은?</b></summary>
  A: 

  - KNN (K-Nearest Neighbors)
    - 분류(Classification) / 회귀(Regression) 알고리즘
    - 지도학습(Supervised Learning) 방식
    - 새로운 데이터가 들어왔을 때, 가장 가까운 K개의 이웃 데이터 레이블을 보고 다수결(분류)이나 평균(회귀)으로 예측
    - 핵심: 거리 계산 (Euclidean, Manhattan 등)
    - 예: 어떤 학생이 새로 전학 왔을 때, 주변 친구들의 전공이 컴퓨터공학이 많으면 이 학생도 컴퓨터공학일 거라 예측
  - K-means
    - 클러스터링(Clustering) 알고리즘
    - 비지도학습(Unsupervised Learning) 방식
    - 데이터들을 K개의 그룹(클러스터)으로 나누는 방법
    - 각 그룹 중심(centroid)을 찾고, 데이터들을 가장 가까운 중심에 할당 → 중심을 업데이트 → 수렴할 때까지 반복
    - 예: 학생들의 전공을 모르지만, 성적과 취미 데이터를 기반으로 3개의 그룹으로 묶어보면 자연스럽게 “컴퓨터공학 / 경영학 / 예술학”과 같은 군집이 생김
</details>
<details>
<summary><b>26. 하이퍼파라미터(Hyperparameter)란? 하이퍼파라미터 튜닝이란?</b></summary>
  A: 

  - 정의 : 하이퍼파라미터는 모델 학습 과정 전에 사람이 직접 설정하는 값, 모델의 구조나 학습 방식을 결정하는 데 중요한 역할을 함
  - 예시
    - 학습률(Learning Rate): 한 번의 업데이트에서 얼마나 크게 가중치를 조정할지
    - 은닉층 개수 / 뉴런 개수: 신경망의 구조를 결정
    - 배치 크기(Batch Size): 한 번에 학습하는 데이터 샘플 수
    - 에포크(Epoch): 전체 데이터를 몇 번 반복해서 학습할지
    - 정규화 계수(L1, L2 λ 값), Dropout 비율 등
  - 하이퍼파라미터 튜닝
    - 하이퍼파라미터의 값을 적절히 조절해서 모델의 성능을 최적화하는 과정
    - 목적
      - 잘못 설정하면 모델이 과적합(Overfitting)하거나 학습이 제대로 안 됨
      - 최적의 값을 찾아야 일반화 성능(새 데이터 예측 능력)이 좋아짐
    - 방법
      - Grid Search: 가능한 조합을 전부 탐색
      - Random Search: 무작위로 하이퍼파라미터 값을 골라 탐색
      - Bayesian Optimization, Hyperband, Optuna: 더 효율적인 자동 탐색 기법
      - **교차 검증(Cross Validation)**과 함께 사용해 일반화 성능 평가
</details>
<details>
<summary><b>27. 머신러닝에서는 왜 데이터 확보가 중요한 것인가?</b></summary>
  A: 

  1. 학습의 재료가 되기 때문
     - 머신러닝은 데이터를 기반으로 패턴을 학습함
     - 충분한 양의 데이터가 있어야 모델이 다양한 상황을 경험하며 일반화 능력을 가질 수 있음
     - 데이터가 부족하면 모델은 훈련 데이터에만 맞춰져서 과적합(Overfitting) 문제가 발생할 수 있음
  2. 데이터의 질이 성능을 좌우하기 때문
     - 아무리 복잡하고 뛰어난 알고리즘이라도 노이즈가 많거나 잘못된 레이블을 가진 데이터로 학습하면 성능이 크게 떨어짐
     - 즉, Garbage in, Garbage out 원리가 그대로 적용됨
  3. 편향(Bias) 문제 방지
     - 데이터가 특정 집단이나 상황에 치우쳐 있으면 모델도 그 편향을 그대로 학습함
     - 예: 얼굴 인식 모델이 특정 인종 데이터에만 집중되어 있으면 다른 인종 인식률이 낮아짐
     - 공정하고 신뢰할 수 있는 AI를 만들기 위해서는 다양하고 균형 잡힌 데이터 확보가 필수
  4. 특성(Feature) 표현력 확보
     - 데이터가 많을수록 더 다양한 특징(feature) 을 학습할 수 있음
     - 이는 단순히 데이터 양뿐 아니라 데이터 다양성(다양한 환경, 조건, 사용자)에 의해서도 결정됨
  5. 튜닝 및 검증 가능성
      - 충분한 데이터가 있어야 훈련/검증/테스트 세트를 나누어 모델의 성능을 공정하게 평가할 수 있음
      - 데이터가 부족하면 모델 성능을 제대로 검증하기 어려움
</details>
<details>
<summary><b>28. 인공지능에서 왜 Bias는 해결할 수 없는 난제인가?</b></summary>
  A: 

  1. 데이터 자체에 내재된 편향
     - AI는 데이터 기반 학습을 함
     - 그런데 현실에서 수집되는 데이터는 환경의 영향을 받아 이미 불완전하거나 불균형함
     - 따라서 데이터를 완벽히 공정하게 만드는 것 자체가 불가능
  2. 정의의 다의성 (What is “Fair”?)
     - Bias를 제거하려면 “공정성”을 정의해야 하지만, 그 기준이 상황마다 다름
     - 어떤 기준을 택하느냐에 따라 다른 집단에서 또 다른 불만족이 생김
     - 즉, 완벽히 만족하는 보편적 공정성 기준은 존재하지 않음
  3. 모델의 복잡성과 비가시성
     - 딥러닝 같은 복잡한 모델은 수억 개의 파라미터로 이뤄져 있어, 편향이 어디서 어떻게 강화되는지 명확히 알기 어려움
     - Explainable AI(XAI) 기법으로 원인을 일부 해석할 수 있지만, 완전히 투명하게 편향을 추적·제거하는 것은 사실상 불가능
  4. 새로운 데이터에서도 반복 발생
     - 설령 현재 데이터에서 Bias를 최대한 줄였다 해도, 새로운 데이터(실시간 입력, 미래 사회 변화 등)에서 다시 새로운 형태의 편향이 나타남
     - 즉, Bias는 정적(static) 문제라기보다 동적(dynamic) 문제이므로 근본적으로 사라지지 않음
  5. 기술적 한계와 사회적 가치의 충돌
     - 편향을 완전히 제거하려는 알고리즘적 시도는 종종 성능 저하나 다른 집단에서의 불평등 심화로 이어짐
     - 기술적으로는 줄일 수 있으나, 사회·문화적 합의 없이는 완전 해결이 불가능
</details>

